{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Team Name : PeErEx\n",
        "\n",
        "Team Leader : Bayu Wicaksono\n",
        "\n",
        "Team Member : \n",
        "1. Mochamad Khaairi\n",
        "2. Muhammad Fikri Kafilli"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## **Overview**\n",
        "\n",
        "This project involves classifying images of T-shirts and hoodies from various online stores in Indonesia into categories based on type and color.\n",
        "\n",
        "## **Dataset**\n",
        "The dataset comprises 1111 images split into training and test sets:\n",
        "\n",
        "### **Training Set:**\n",
        "- **train**: Contains 777 images of T-shirts and hoodies in 5 different colors.\n",
        "- **train.csv**: CSV file containing labels for each training image.\n",
        "  - **id**: Unique ID for each training image.\n",
        "  - **jenis**: Type of clothing (0: T-shirt, 1: Hoodie).\n",
        "  - **warna**: Color of clothing (0: Red, 1: Yellow, 2: Blue, 3: Black, 4: White).\n",
        "\n",
        "### **Test Set:**\n",
        "- **test**: Contains 334 images for classification."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Requirement\n",
        "pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121 pandas numpy matplotlib seaborn opencv-python Pillow transformers scikit-learn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Import Library"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "27xFKy8cHO4B"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import cv2\n",
        "from PIL import Image\n",
        "import torch\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import Dataset, DataLoader, random_split"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## EDA and Data Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Load Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We begin by loading the training data from the specified directory and reading the accompanying CSV file containing labels. This sets up our environment for further data processing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 777 entries, 0 to 776\n",
            "Data columns (total 3 columns):\n",
            " #   Column  Non-Null Count  Dtype\n",
            "---  ------  --------------  -----\n",
            " 0   id      777 non-null    int64\n",
            " 1   jenis   777 non-null    int64\n",
            " 2   warna   777 non-null    int64\n",
            "dtypes: int64(3)\n",
            "memory usage: 18.3 KB\n"
          ]
        }
      ],
      "source": [
        "# Image Directory\n",
        "data_image = 'hology/data/train/train'\n",
        "\n",
        "# Load CSV file\n",
        "data = pd.read_csv('hology/data/train.csv')\n",
        "data.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Check for Duplicate Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Next, we preprocess the images to check for any duplicates. We need to load every images and then save it as pixels value, so we can compare it. The process includes, each image is read using cv2 library, and then because cv2 read images in BGR (blue, green, red) colors, we need to converted to RGB, do resized, flattened into a 1D array, and save pixels value along with its id into dataframe. This transformation allows us to compare images efficiently."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>pixels</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>242 242 242 242 242 242 242 242 242 242 242 24...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>10</td>\n",
              "      <td>255 255 255 255 255 255 255 255 255 255 255 25...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>100</td>\n",
              "      <td>250 250 250 250 250 250 250 250 250 250 250 25...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>101</td>\n",
              "      <td>255 255 255 255 255 255 255 255 255 255 255 25...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>102</td>\n",
              "      <td>153 104 45 151 103 41 149 101 38 148 100 35 14...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    id                                             pixels\n",
              "0    1  242 242 242 242 242 242 242 242 242 242 242 24...\n",
              "1   10  255 255 255 255 255 255 255 255 255 255 255 25...\n",
              "2  100  250 250 250 250 250 250 250 250 250 250 250 25...\n",
              "3  101  255 255 255 255 255 255 255 255 255 255 255 25...\n",
              "4  102  153 104 45 151 103 41 149 101 38 148 100 35 14..."
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "image_data = []\n",
        "image_ids = []\n",
        "\n",
        "# extract id and pixel from image to dataframe\n",
        "for img_file in os.listdir(data_image):\n",
        "    img_path = os.path.join(data_image, img_file)\n",
        "\n",
        "    if os.path.isfile(img_path):\n",
        "        # get image id from file name\n",
        "        img_id = np.int64(os.path.splitext(img_file)[0])\n",
        "\n",
        "        # load image with BGR color\n",
        "        image = cv2.imread(img_path, cv2.IMREAD_COLOR)\n",
        "\n",
        "        if image is not None:\n",
        "            # convert from BGR to RGB\n",
        "            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "            # resize image\n",
        "            image = cv2.resize(image, (244, 244))\n",
        "\n",
        "            # flatten image to 1D array and convert to string\n",
        "            image_flat = ' '.join(map(str, image.flatten()))\n",
        "\n",
        "            # save pixel and image id\n",
        "            image_data.append(image_flat)\n",
        "            image_ids.append(img_id)\n",
        "\n",
        "df = pd.DataFrame({\n",
        "    'id': image_ids,\n",
        "    'pixels': image_data\n",
        "})\n",
        "\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We then check for any duplicate images based on their pixel values. This ensures the uniqueness of the images used in our training and evaluation processes. Subset is used to ensure that duplicate checking is only performed for pixel values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of duplicate rows': 5\n"
          ]
        }
      ],
      "source": [
        "duplicate_count = df.duplicated(subset='pixels').sum()\n",
        "\n",
        "print(f\"Number of duplicate rows': {duplicate_count}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>pixels</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>412</th>\n",
              "      <td>470</td>\n",
              "      <td>255 255 255 255 255 255 255 255 255 255 255 25...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>570</th>\n",
              "      <td>612</td>\n",
              "      <td>220 218 219 220 218 219 220 218 219 220 218 21...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>674</th>\n",
              "      <td>706</td>\n",
              "      <td>255 255 255 255 255 255 255 255 255 255 255 25...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>725</th>\n",
              "      <td>752</td>\n",
              "      <td>255 255 255 255 255 255 255 255 255 255 255 25...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>738</th>\n",
              "      <td>764</td>\n",
              "      <td>255 255 255 255 255 255 255 255 255 255 255 25...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      id                                             pixels\n",
              "412  470  255 255 255 255 255 255 255 255 255 255 255 25...\n",
              "570  612  220 218 219 220 218 219 220 218 219 220 218 21...\n",
              "674  706  255 255 255 255 255 255 255 255 255 255 255 25...\n",
              "725  752  255 255 255 255 255 255 255 255 255 255 255 25...\n",
              "738  764  255 255 255 255 255 255 255 255 255 255 255 25..."
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df[df.duplicated(subset='pixels')]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We store duplicate IDs in a list. This list will later be used to filter out and delete duplicate rows from the dataframe, ensuring the dataset contains only unique images."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "id_duplicate = df[df.duplicated(subset='pixels')]['id'].tolist()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Clothing Type Data Distribution"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We map the numerical labels for clothing types to their corresponding string labels (0: Kaos, 1: Hoodie). We then calculate the count of each type and create a dataframe to store these counts."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>jenis</th>\n",
              "      <th>number</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Kaos</td>\n",
              "      <td>476</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Hoodie</td>\n",
              "      <td>301</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    jenis  number\n",
              "0    Kaos     476\n",
              "1  Hoodie     301"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "labels = {0:'Kaos', 1:'Hoodie'}\n",
        "counts = data['jenis'].value_counts(sort=True).reset_index()\n",
        "counts.columns = ['jenis', 'number']\n",
        "counts['jenis'] = counts['jenis'].map(labels)\n",
        "counts"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To understand the distribution of different categories in our dataset, we create a bar plot. The plot displays the count of each category (T-shirt and Hoodie), which helps in visualizing how balanced the dataset is."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 600x400 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "counts_sorted = counts.sort_values(by='number', ascending=True)\n",
        "colors = sns.color_palette(\"Blues\", len(counts_sorted))\n",
        "\n",
        "plt.figure(figsize=(6, 4))\n",
        "sns.barplot(x=counts_sorted.jenis, y=counts_sorted.number, palette=colors, hue=counts_sorted.jenis)\n",
        "plt.title('Category Distribution')\n",
        "plt.ylabel('Count', fontsize=12)\n",
        "plt.xlabel('Categories', fontsize=12)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Clothing Color Data Distribution"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "For color labels, we do the same thing as we did for clothing type"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>warna</th>\n",
              "      <th>number</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>hitam</td>\n",
              "      <td>234</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>biru</td>\n",
              "      <td>162</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>putih</td>\n",
              "      <td>140</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>kuning</td>\n",
              "      <td>125</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>merah</td>\n",
              "      <td>116</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    warna  number\n",
              "0   hitam     234\n",
              "1    biru     162\n",
              "2   putih     140\n",
              "3  kuning     125\n",
              "4   merah     116"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "labels = {0:'merah', 1:'kuning', 2:'biru', 3:'hitam', 4:'putih'}\n",
        "counts = data['warna'].value_counts(sort=True).reset_index()\n",
        "counts.columns = ['warna', 'number']\n",
        "counts['warna'] = counts['warna'].map(labels)\n",
        "counts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 800x400 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "counts_sorted = counts.sort_values(by='number', ascending=True)\n",
        "\n",
        "colors = sns.color_palette(\"Blues\", len(counts_sorted))\n",
        "\n",
        "plt.figure(figsize=(8, 4))\n",
        "sns.barplot(x=counts_sorted.warna, y=counts_sorted.number, palette=colors, hue=counts_sorted.warna)\n",
        "plt.title('Color Distribution')\n",
        "plt.ylabel('Count', fontsize=12)\n",
        "plt.xlabel('Colors', fontsize=12)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Data Training Preparation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We define a function to locate the image file based on its ID. This helps in dynamically finding the correct image file in the specified directory."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "# find image path based image id\n",
        "def find_file_path(img_dir, img_id):\n",
        "    for ext in ['jpg', 'png', 'jpeg']:\n",
        "        file_path = os.path.join(img_dir, f'{img_id}.{ext}')\n",
        "        if os.path.exists(file_path):\n",
        "            return file_path.replace('\\\\', '/')\n",
        "    return None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We create a custom dataset class using PyTorch. This class handles the loading of image data and corresponding labels, as well as applying any required transformations to the images."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "M8OZdzrazGTE"
      },
      "outputs": [],
      "source": [
        "class ClothingDataset(Dataset):\n",
        "    def __init__(self, csv_file, img_dir, transform=None):\n",
        "        self.data = pd.read_csv(csv_file)\n",
        "        self.img_dir = img_dir\n",
        "        self.transform = transform\n",
        "        # delete duplicate data\n",
        "        # self.data = self.data[~self.data['id'].isin(id_duplicate)].reset_index(drop=True)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # get id from csv file\n",
        "        img_id = str(self.data.iloc[idx, 0])\n",
        "        # get image path\n",
        "        img_path = find_file_path(self.img_dir, img_id)\n",
        "\n",
        "        # Periksa apakah file ada\n",
        "        if not os.path.exists(img_path):\n",
        "            raise FileNotFoundError(f\"Gambar {img_path} tidak ditemukan.\")\n",
        "\n",
        "        # convert image to RGB\n",
        "        image = Image.open(img_path).convert(\"RGB\")\n",
        "\n",
        "        # get label data from csv file\n",
        "        jenis = torch.tensor(self.data.iloc[idx, 1], dtype=torch.long)\n",
        "        warna = torch.tensor(self.data.iloc[idx, 2], dtype=torch.long)\n",
        "\n",
        "        # apply transformation to image\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, jenis, warna"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We defines a transformation pipeline for image processing using PyTorch's transforms module, including resized images into 224x224 pixels to macth with input size of model. Then we load the training data, and split it into training and validation sets. We use data loaders to facilitate efficient batching and shuffling during training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "fC3y5kwBzMWK"
      },
      "outputs": [],
      "source": [
        "# define transformation for image\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])\n",
        "])\n",
        "\n",
        "# load data training\n",
        "full_train_dataset = ClothingDataset(csv_file=\"hology/data/train.csv\", img_dir=\"hology/data/train/train\", transform=transform)\n",
        "\n",
        "# set seed\n",
        "torch.manual_seed(42)\n",
        "\n",
        "# determine data training and validation sizes (80% and 20%)\n",
        "train_size = int(0.8 * len(full_train_dataset))\n",
        "val_size = len(full_train_dataset) - train_size\n",
        "\n",
        "# set seed\n",
        "generator = torch.Generator().manual_seed(42)\n",
        "\n",
        "# split dataset\n",
        "train_dataset, val_dataset = random_split(full_train_dataset, [train_size, val_size], generator=generator)\n",
        "\n",
        "# create DataLoader for data training and validation\n",
        "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Model Development"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We define a custom neural network using the Vision Transformer (ViT) model from the Transformers library by Hugging Face. The model is fine-tuned for multi-label classification to predict both the type of clothing and its color.\n",
        "\n",
        "- We load a pre-trained ViT model (`vit-base-patch16-224`) and create two custom classification heads.\n",
        "- `vit-base-patch16-224` is specific ViT model with the input image is divided into patches of 16x16 pixels and resolution of the input image, specifically 224x224 pixels.\n",
        "- The `classifier_jenis` head is for binary classification of clothing type (T-shirt or Hoodie).\n",
        "- The `classifier_warna` head is for multi-class classification of clothing color (5 colors)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "611fd30bf3d94d818489b6e7f7bd12c9",
            "fe767983311349a99a065cb860e875d6",
            "35d8f35e76c040468b45fee1592fd28f",
            "db3464bc619d4975a614bacb4c65881d",
            "598893d59bc344aaabfbf64cdbac3f87",
            "fc370bbbfa224832b4aca4b42e56949b",
            "61adb54bd5d04d34b013a7c36db323e9",
            "28a9df83393a4c3c81093b18ca128579",
            "812f724a457d420b9802226582f73d49",
            "d6953d5c047343bb979fdeb19462ce50",
            "6485153a12144beb91352ca0bcad2e5b",
            "4b90936a3a2546acae25afc2b228e7e0",
            "f0dc778235fe43d0a1391fb1206690ea",
            "698887ae0f3540cfaacc69819144c967",
            "f394db09e39043209b39590f9be4d55f",
            "aaa8914978f945c9bdadb0da711609a0",
            "e7ef50bdfd0740078530eaaabe6bdd9a",
            "e1367c7b28ab4801ac331065aff0436e",
            "3b59d7c40db14f7cac416f4e4ca78ad7",
            "b805c212b79247aebd62e594bb4ba4b1",
            "779c90e6f0f2410d9af4b35eaee93a57",
            "18db0e22f4294266971c579cbe2e023f"
          ]
        },
        "id": "5fo7dd9ZzW4-",
        "outputId": "6f086aab-11ed-47fa-b665-1349426aa502"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\User\\Documents\\Code\\env\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "MultiLabelViT(\n",
              "  (vit): ViTModel(\n",
              "    (embeddings): ViTEmbeddings(\n",
              "      (patch_embeddings): ViTPatchEmbeddings(\n",
              "        (projection): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
              "      )\n",
              "      (dropout): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (encoder): ViTEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0-11): 12 x ViTLayer(\n",
              "          (attention): ViTSdpaAttention(\n",
              "            (attention): ViTSdpaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.0, inplace=False)\n",
              "            )\n",
              "            (output): ViTSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.0, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): ViTIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): ViTOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "    (pooler): ViTPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (classifier_jenis): Linear(in_features=768, out_features=1, bias=True)\n",
              "  (classifier_warna): Linear(in_features=768, out_features=5, bias=True)\n",
              ")"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from transformers import ViTModel\n",
        "import torch.nn as nn\n",
        "\n",
        "class MultiLabelViT(nn.Module):\n",
        "    def __init__(self, pretrained_model_name='google/vit-base-patch16-224'):\n",
        "        super(MultiLabelViT, self).__init__()\n",
        "        # load pre-trained model\n",
        "        self.vit = ViTModel.from_pretrained(pretrained_model_name)\n",
        "\n",
        "        # get the size (dimension) of the hidden layer representation\n",
        "        hidden_size = self.vit.config.hidden_size\n",
        "\n",
        "        # define custom classification heads\n",
        "        self.classifier_jenis = nn.Linear(hidden_size, 1) # for clothing type, binary classification\n",
        "        self.classifier_warna = nn.Linear(hidden_size, 5) # for clothing color, multi-class classification\n",
        "\n",
        "    def forward(self, pixel_values):\n",
        "        # process the input image through the ViT model and get the output\n",
        "        outputs = self.vit(pixel_values=pixel_values)\n",
        "\n",
        "        # taking the output from the last hidden layer\n",
        "        last_hidden_state = outputs.last_hidden_state\n",
        "\n",
        "        # get representation of classification token ([CLS])\n",
        "        cls_token = last_hidden_state[:, 0, :]\n",
        "\n",
        "        # generate logits for each label\n",
        "        jenis_logits = self.classifier_jenis(cls_token)\n",
        "        warna_logits = self.classifier_warna(cls_token)\n",
        "\n",
        "        return jenis_logits, warna_logits\n",
        "\n",
        "\n",
        "# init model\n",
        "model = MultiLabelViT()\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We calculate the total number of parameters and the number of trainable parameters in the model. This provides insights into the complexity of the model and the training process."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SkhNwlPj8j-U",
        "outputId": "2f571367-8e5f-4a8f-9af8-f65adea5990d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "num_params = 86,393,862 | trainable_params = 86,393,862\n"
          ]
        }
      ],
      "source": [
        "# see the number of parameters to be updated\n",
        "num_params = sum([p.numel() for p in model.parameters()])\n",
        "trainable_params = sum([p.numel() for p in model.parameters() if p.requires_grad])\n",
        "\n",
        "print(f\"{num_params = :,} | {trainable_params = :,}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Evaluation Model Process"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We define a function to evaluate the performance of our model. The function calculates several metrics including accuracy and exact match ratio, and uses the precision-recall curve to determine the optimal threshold for binary classification.\n",
        "\n",
        "- **Accuracy**: Measures how often the model correctly predicts the clothing type and color.\n",
        "- **Exact Match Ratio**: The percentage of samples where both clothing type and color are predicted correctly.\n",
        "- **Precision-Recall Curve**: Helps determine the optimal threshold for binary classification of clothing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "SU-atsa6zX81"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import precision_recall_curve\n",
        "import numpy as np\n",
        "\n",
        "# evaluation model using accuracy and exact match ratio\n",
        "def evaluate_model(model, dataloader):\n",
        "    model.eval()\n",
        "    total_samples = 0\n",
        "    exact_match_count = 0\n",
        "    correct_jenis = 0\n",
        "    correct_warna = 0\n",
        "    all_true_labels = []\n",
        "    all_predicted_probs = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, jenis, warna in dataloader:\n",
        "            images = images.to(device)\n",
        "            jenis = jenis.to(device).float().view(-1)\n",
        "            warna = warna.to(device)\n",
        "\n",
        "            # forward pass\n",
        "            jenis_logits, warna_logits = model(images)\n",
        "\n",
        "            # apply sigmoid to convert logits to probabilities for binary classification\n",
        "            predicted_jenis = torch.sigmoid(jenis_logits)\n",
        "\n",
        "            # store predicted probabilities and true labels for ROC-AUC calculation\n",
        "            all_predicted_probs.extend(predicted_jenis.cpu().numpy())\n",
        "            all_true_labels.extend(jenis.cpu().numpy())\n",
        "\n",
        "            # apply threshold of 0.5\n",
        "            predicted_jenis = (predicted_jenis > 0.5).long().view(-1)\n",
        "\n",
        "\n",
        "            # prediction for multi-class classification\n",
        "            _, predicted_warna = torch.max(warna_logits, 1)\n",
        "\n",
        "            # calculate accuracy for each label\n",
        "            correct_jenis += (predicted_jenis == jenis.long()).sum().item()\n",
        "            correct_warna += (predicted_warna == warna).sum().item()\n",
        "\n",
        "            # calculate exact match ratio\n",
        "            exact_match_count += ((predicted_jenis == jenis.long()) & (predicted_warna == warna)).sum().item()\n",
        "            total_samples += jenis.size(0)\n",
        "\n",
        "    # calculate all evaluation matrix\n",
        "    jenis_accuracy = correct_jenis / total_samples * 100\n",
        "    warna_accuracy = correct_warna / total_samples * 100\n",
        "    exact_match_ratio = exact_match_count / total_samples * 100\n",
        "\n",
        "    # calculate precision-Recall curve\n",
        "    precision, recall, thresholds = precision_recall_curve(all_true_labels, all_predicted_probs)\n",
        "\n",
        "    # calculate optimal threshold\n",
        "    f1_scores = 2 * (precision * recall) / (precision + recall + 1e-7)\n",
        "    optimal_idx = np.argmax(f1_scores)\n",
        "    optimal_threshold = thresholds[optimal_idx]\n",
        "\n",
        "    print(f'Optimal Threshold: {optimal_threshold:.4f}')\n",
        "    print(f'Akurasi Jenis: {jenis_accuracy:.2f}%, Akurasi Warna: {warna_accuracy:.2f}%, Exact Match Ratio (EMR): {exact_match_ratio:.2f}%')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Training Model Process"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We define the loss functions and optimizer for our multi-label classification model. The training loop iterates through the dataset, computes losses for both labels, and updates the model parameters to minimize the combined loss. An additional penalty for the exact match ratio (EMR) ensures the model is penalized for incorrect predictions.\n",
        "\n",
        "- **Loss Functions**:\n",
        "  - `lossFunc_jenis`: Binary Cross-Entropy Loss for clothing type.\n",
        "  - `lossFunc_warna`: Cross-Entropy Loss for clothing color.\n",
        "- **Optimizer**: Adam optimizer with a learning rate of 1e-4.\n",
        "- **Alpha**: Penalty weight for EMR to ensure correct multi-label predictions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\User\\Documents\\Code\\env\\lib\\site-packages\\transformers\\models\\vit\\modeling_vit.py:261: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:555.)\n",
            "  context_layer = torch.nn.functional.scaled_dot_product_attention(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/10], Loss: 0.733513\n",
            "Optimal Threshold: 0.6390\n",
            "Akurasi Jenis: 98.72%, Akurasi Warna: 97.44%, Exact Match Ratio (EMR): 96.15%\n",
            "Epoch [2/10], Loss: 0.104613\n",
            "Optimal Threshold: 0.5454\n",
            "Akurasi Jenis: 100.00%, Akurasi Warna: 98.72%, Exact Match Ratio (EMR): 98.72%\n",
            "Epoch [3/10], Loss: 0.061821\n",
            "Optimal Threshold: 0.6511\n",
            "Akurasi Jenis: 100.00%, Akurasi Warna: 98.08%, Exact Match Ratio (EMR): 98.08%\n",
            "Epoch [4/10], Loss: 0.048526\n",
            "Optimal Threshold: 0.2465\n",
            "Akurasi Jenis: 99.36%, Akurasi Warna: 100.00%, Exact Match Ratio (EMR): 99.36%\n",
            "Epoch [5/10], Loss: 0.044127\n",
            "Optimal Threshold: 0.3535\n",
            "Akurasi Jenis: 99.36%, Akurasi Warna: 100.00%, Exact Match Ratio (EMR): 99.36%\n",
            "Epoch [6/10], Loss: 0.042815\n",
            "Optimal Threshold: 0.3327\n",
            "Akurasi Jenis: 99.36%, Akurasi Warna: 100.00%, Exact Match Ratio (EMR): 99.36%\n",
            "Epoch [7/10], Loss: 0.042093\n",
            "Optimal Threshold: 0.3221\n",
            "Akurasi Jenis: 99.36%, Akurasi Warna: 100.00%, Exact Match Ratio (EMR): 99.36%\n",
            "Epoch [8/10], Loss: 0.041657\n",
            "Optimal Threshold: 0.3037\n",
            "Akurasi Jenis: 99.36%, Akurasi Warna: 100.00%, Exact Match Ratio (EMR): 99.36%\n",
            "Epoch [9/10], Loss: 0.041303\n",
            "Optimal Threshold: 0.3034\n",
            "Akurasi Jenis: 99.36%, Akurasi Warna: 100.00%, Exact Match Ratio (EMR): 99.36%\n",
            "Epoch [10/10], Loss: 0.041129\n",
            "Optimal Threshold: 0.2829\n",
            "Akurasi Jenis: 99.36%, Akurasi Warna: 100.00%, Exact Match Ratio (EMR): 99.36%\n"
          ]
        }
      ],
      "source": [
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "# define training arguments\n",
        "lossFunc_jenis = nn.BCEWithLogitsLoss()  # binary loss for 'jenis'\n",
        "lossFunc_warna = nn.CrossEntropyLoss()  # multi-class loss for 'warna'\n",
        "alpha = 0.1  # penalty weight for EMR\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
        "\n",
        "# training loop\n",
        "num_epochs = 10\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    total_loss_epoch = 0\n",
        "\n",
        "    for images, jenis, warna in train_loader:\n",
        "        images, jenis, warna = images.to(device), jenis.to(device).float(), warna.to(device).float()\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # forward pass\n",
        "        jenis_logits, warna_logits = model(images)\n",
        "\n",
        "        # compute the losses for each label\n",
        "        loss_jenis = lossFunc_jenis(jenis_logits, jenis.unsqueeze(1))\n",
        "        loss_warna = lossFunc_warna(warna_logits, warna.long())\n",
        "\n",
        "        # custom loss for exact match\n",
        "        _, predicted_jenis = torch.max(jenis_logits, dim=1)\n",
        "        _, predicted_warna = torch.max(warna_logits, dim=1)\n",
        "        correct_predictions = (predicted_jenis == jenis) & (predicted_warna == warna)\n",
        "        penalti_EMR = (1 - correct_predictions.float()).mean()\n",
        "\n",
        "        # combine losses\n",
        "        total_loss = loss_jenis + loss_warna + alpha * penalti_EMR\n",
        "        \n",
        "        # backward pass\n",
        "        total_loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss_epoch += total_loss.item()\n",
        "\n",
        "    avg_loss = total_loss_epoch / len(train_loader)\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.6f}\")\n",
        "\n",
        "    # evaluate model\n",
        "    evaluate_model(model, val_loader)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Save Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "cw40_StgzwhX"
      },
      "outputs": [],
      "source": [
        "# torch.save(model.state_dict(), \"hology/model/multilabel_vit-1.pth\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Inference Process"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_11040\\84290560.py:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load(\"multilabel_vit-1.pth\", map_location=device))\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "MultiLabelViT(\n",
              "  (vit): ViTModel(\n",
              "    (embeddings): ViTEmbeddings(\n",
              "      (patch_embeddings): ViTPatchEmbeddings(\n",
              "        (projection): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
              "      )\n",
              "      (dropout): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (encoder): ViTEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0-11): 12 x ViTLayer(\n",
              "          (attention): ViTSdpaAttention(\n",
              "            (attention): ViTSdpaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.0, inplace=False)\n",
              "            )\n",
              "            (output): ViTSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.0, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): ViTIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): ViTOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "    (pooler): ViTPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (classifier_jenis): Linear(in_features=768, out_features=1, bias=True)\n",
              "  (classifier_warna): Linear(in_features=768, out_features=5, bias=True)\n",
              ")"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# # Load the model\n",
        "# model = MultiLabelViT()\n",
        "# model.load_state_dict(torch.load(\"hology/model/multilabel_vit-1.pth\", map_location=device))\n",
        "# model.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We define function to predict data test by generates a CSV file for save result of prediction later. This function begins by listing all files within the img_dir directory and extracts the image IDs by removing the file extensions from each filename. It then creates a DataFrame."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "iF8353pz2nlM"
      },
      "outputs": [],
      "source": [
        "# create csv file for test data\n",
        "def create_test_csv(img_dir, output_csv):\n",
        "    # list all files in the test directory\n",
        "    img_files = os.listdir(img_dir)\n",
        "\n",
        "    # extract id from filename\n",
        "    img_ids = [os.path.splitext(f)[0] for f in img_files]\n",
        "\n",
        "    # create dataframe for test data with column jenis and warna is fill with null values\n",
        "    df = pd.DataFrame({\n",
        "        'id': img_ids,\n",
        "        'jenis': -1,\n",
        "        'warna': -1\n",
        "    })\n",
        "    df = df.sort_values(by='id', ascending=True)\n",
        "    # save dataframe to csv\n",
        "    df.to_csv(output_csv, index=False)\n",
        "\n",
        "# Create the CSV file for test data\n",
        "create_test_csv('hology/data/test/test', 'hology/data/test_predictions.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This function loads a trained model to predict labels for a test dataset. It reads images and their IDs from a specified directory and loads them using a custom ClothingDataset class. The function prepares batches of test data, runs them through the model in evaluation mode, and generates predictions for both jenis and warna attributes. For binary classification (jenis), it applies a threshold to sigmoid outputs, and for multi-class color classification (warna), it selects the class with the highest logit. The predictions are then stored and used to update the test CSV file, which is saved with sorted entries by ID."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yvE6qi7W4V1_",
        "outputId": "2c776d28-06cd-4f77-9799-881bb71a9492"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_11040\\3280458754.py:25: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
            "  predictions.append((int(jenis), int(warna)))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Predictions saved to test_predictions.csv\n"
          ]
        }
      ],
      "source": [
        "def predict_and_save(model, test_csv, img_dir, output_csv, transform, batch_size=16, threshold=0.5):\n",
        "    # Load test dataset and create dataloader\n",
        "    test_dataset = ClothingDataset(csv_file=test_csv, img_dir=img_dir, transform=transform)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    model.eval()\n",
        "    predictions = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, _, _ in test_loader:\n",
        "            images = images.to(device)\n",
        "\n",
        "            # get predictions\n",
        "            jenis_logits, warna_logits = model(images)\n",
        "\n",
        "            # predict for jenis label\n",
        "            predicted_jenis = torch.sigmoid(jenis_logits)\n",
        "            predicted_jenis = (predicted_jenis > threshold).long()  # Apply optimal threshold values\n",
        "            \n",
        "            # predict for warna label\n",
        "            _, predicted_warna = torch.max(warna_logits, 1)\n",
        "\n",
        "            # store predictions\n",
        "            for jenis, warna in zip(predicted_jenis.cpu().numpy(), predicted_warna.cpu().numpy()):\n",
        "                predictions.append((int(jenis), int(warna)))\n",
        "\n",
        "    # load test CSV and update jenis and warna column with predictions\n",
        "    df = pd.read_csv(test_csv)\n",
        "    df['jenis'] = [p[0] for p in predictions]\n",
        "    df['warna'] = [p[1] for p in predictions]\n",
        "\n",
        "    # save csv\n",
        "    df = df.sort_values(by='id', ascending=True)\n",
        "    df.to_csv(output_csv, index=False)\n",
        "    print(f\"Predictions saved to {output_csv}\")\n",
        "\n",
        "\n",
        "# Run prediction and save the results\n",
        "opt_threshold = 0.7\n",
        "predict_and_save(model,\n",
        "                 test_csv='hology/data/test_predictions.csv',\n",
        "                 img_dir='hology/data/test/test',\n",
        "                 output_csv='hology/data/test_predictions.csv',\n",
        "                 transform=transform, threshold=opt_threshold)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.5"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "18db0e22f4294266971c579cbe2e023f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "28a9df83393a4c3c81093b18ca128579": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "35d8f35e76c040468b45fee1592fd28f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_28a9df83393a4c3c81093b18ca128579",
            "max": 69665,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_812f724a457d420b9802226582f73d49",
            "value": 69665
          }
        },
        "3b59d7c40db14f7cac416f4e4ca78ad7": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4b90936a3a2546acae25afc2b228e7e0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f0dc778235fe43d0a1391fb1206690ea",
              "IPY_MODEL_698887ae0f3540cfaacc69819144c967",
              "IPY_MODEL_f394db09e39043209b39590f9be4d55f"
            ],
            "layout": "IPY_MODEL_aaa8914978f945c9bdadb0da711609a0"
          }
        },
        "598893d59bc344aaabfbf64cdbac3f87": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "611fd30bf3d94d818489b6e7f7bd12c9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_fe767983311349a99a065cb860e875d6",
              "IPY_MODEL_35d8f35e76c040468b45fee1592fd28f",
              "IPY_MODEL_db3464bc619d4975a614bacb4c65881d"
            ],
            "layout": "IPY_MODEL_598893d59bc344aaabfbf64cdbac3f87"
          }
        },
        "61adb54bd5d04d34b013a7c36db323e9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6485153a12144beb91352ca0bcad2e5b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "698887ae0f3540cfaacc69819144c967": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3b59d7c40db14f7cac416f4e4ca78ad7",
            "max": 346293852,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b805c212b79247aebd62e594bb4ba4b1",
            "value": 346293852
          }
        },
        "779c90e6f0f2410d9af4b35eaee93a57": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "812f724a457d420b9802226582f73d49": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "aaa8914978f945c9bdadb0da711609a0": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b805c212b79247aebd62e594bb4ba4b1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d6953d5c047343bb979fdeb19462ce50": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "db3464bc619d4975a614bacb4c65881d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d6953d5c047343bb979fdeb19462ce50",
            "placeholder": "",
            "style": "IPY_MODEL_6485153a12144beb91352ca0bcad2e5b",
            "value": "69.7k/69.7k[00:00&lt;00:00,3.61MB/s]"
          }
        },
        "e1367c7b28ab4801ac331065aff0436e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e7ef50bdfd0740078530eaaabe6bdd9a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f0dc778235fe43d0a1391fb1206690ea": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e7ef50bdfd0740078530eaaabe6bdd9a",
            "placeholder": "",
            "style": "IPY_MODEL_e1367c7b28ab4801ac331065aff0436e",
            "value": "model.safetensors:100%"
          }
        },
        "f394db09e39043209b39590f9be4d55f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_779c90e6f0f2410d9af4b35eaee93a57",
            "placeholder": "",
            "style": "IPY_MODEL_18db0e22f4294266971c579cbe2e023f",
            "value": "346M/346M[00:01&lt;00:00,224MB/s]"
          }
        },
        "fc370bbbfa224832b4aca4b42e56949b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fe767983311349a99a065cb860e875d6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fc370bbbfa224832b4aca4b42e56949b",
            "placeholder": "",
            "style": "IPY_MODEL_61adb54bd5d04d34b013a7c36db323e9",
            "value": "config.json:100%"
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
